{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv4LkdUNqito"
      },
      "outputs": [],
      "source": [
        "# AI MVP Project from datatec.studio\n",
        "!pip install transformers torch accelerate bitsandbytes langchain langchain-community\n",
        "!pip install -U sentence-transformers chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wiKf97Gq7eB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "\n",
        "import time\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvkne0cVA7Zj"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhJNehnaJL9z"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(quantize=True, bits=4)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config=quantization_config,  device_map='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN6d98L8BCdJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CaxahtqsCvB"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    repetition_penalty=1.2,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erc4SygOs1q0"
      },
      "outputs": [],
      "source": [
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZepdEOVqhWk"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# load the document and split it into chunks\n",
        "loader = TextLoader(\"./demo.txt\", encoding='utf-8')\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg1ZDw1FcG3I"
      },
      "outputs": [],
      "source": [
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800, chunk_overlap=80, separators=['\\n\\n', '\\n', '.']\n",
        ")\n",
        "document_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqD9SKuddYi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzX64W8PqiT5"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformerEmbeddings(model_name='BAAI/bge-large-en-v1.5')\n",
        "chroma_db = Chroma.from_documents(document_chunks, embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvvlr8pUpC1V"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o9JzOzFTXjg"
      },
      "outputs": [],
      "source": [
        "# Use Prompt only once\n",
        "import time\n",
        "\n",
        "retriever = chroma_db.as_retriever()\n",
        "\n",
        "# Create question answer chain\n",
        "qa_chain = RetrievalQA.from_chain_type(mistral_llm, retriever=retriever)\n",
        "\n",
        "while True:\n",
        "    # Ask questions to chatbot\n",
        "    # Do you know language DtsDummyLanguage?\n",
        "    # How to use it for web development?\n",
        "    question = input(\"Please enter your question (or 'quit' to stop): \")\n",
        "\n",
        "    if question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "    response = qa_chain({\"query\": question})\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_time = int(end_time - start_time)\n",
        "\n",
        "    print(response['result'])\n",
        "    print(f\"Total calculation time: {total_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajKCjmEw23po"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7cvLZQ7hdNS"
      },
      "outputs": [],
      "source": [
        "# Use Prompt twice\n",
        "\n",
        "\n",
        "while True:\n",
        "    # Ask questions to chatbot\n",
        "    # Do you know language DtsDummyLanguage?\n",
        "    # How to use it for web development?\n",
        "    question = input(\"Please enter your question (or 'quit' to stop): \")\n",
        "\n",
        "    if question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "    similar_search_result = chroma_db.similarity_search(question)\n",
        "    chroma_db_for_prompt = Chroma.from_documents(similar_search_result, embedding_model)\n",
        "    retriever = chroma_db_for_prompt.as_retriever()\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(mistral_llm, retriever=retriever)\n",
        "    response = qa_chain({\"query\": question})\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = int(end_time - start_time)\n",
        "\n",
        "    print(response['result'])\n",
        "    print(f\"Total calculation time: {total_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5NxhSQG1jaC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
